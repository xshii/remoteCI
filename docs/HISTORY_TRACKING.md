# Remote CI å†å²ä»»åŠ¡è®°å½•é—®é¢˜åˆ†æä¸è§£å†³æ–¹æ¡ˆ

## ğŸ” é—®é¢˜æè¿°

å½“å‰ç³»ç»Ÿ**æ— æ³•æ˜¾ç¤ºå†å²ä»»åŠ¡è®°å½•**ï¼ŒWebç•Œé¢åªèƒ½çœ‹åˆ°æ­£åœ¨æ‰§è¡Œæˆ–æ’é˜Ÿä¸­çš„ä»»åŠ¡ã€‚ä¸€æ—¦ä»»åŠ¡å®Œæˆï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰ï¼Œå®ƒå°±ä»ä»»åŠ¡åˆ—è¡¨ä¸­æ¶ˆå¤±äº†ã€‚

## ğŸ“Š å½“å‰å®ç°åˆ†æ

### ä»£ç å®ç°ï¼ˆapp.py ç¬¬230-253è¡Œï¼‰

```python
@app.route('/api/jobs', methods=['GET'])
@require_auth
def list_jobs():
    """åˆ—å‡ºæœ€è¿‘çš„ä»»åŠ¡"""
    # ä»Celeryè·å–æ´»è·ƒä»»åŠ¡
    inspect = celery_app.control.inspect()

    active_tasks = inspect.active() or {}      # æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡
    scheduled_tasks = inspect.scheduled() or {}  # å·²è°ƒåº¦çš„ä»»åŠ¡
    reserved_tasks = inspect.reserved() or {}    # å·²ä¿ç•™çš„ä»»åŠ¡

    # âš ï¸ åªæ”¶é›†æ´»è·ƒä»»åŠ¡ï¼Œä¸åŒ…æ‹¬å·²å®Œæˆçš„ä»»åŠ¡
    jobs = []
    for worker_tasks in [active_tasks, scheduled_tasks, reserved_tasks]:
        for worker, tasks in worker_tasks.items():
            for task in tasks:
                job_info = get_job_info(task['id'])
                jobs.append(job_info)

    return jsonify({
        'jobs': jobs,
        'total': len(jobs)
    })
```

### é—®é¢˜åˆ†æ

| ä»»åŠ¡çŠ¶æ€ | æ˜¯å¦æ˜¾ç¤º | åŸå›  |
|---------|---------|------|
| æ’é˜Ÿä¸­ï¼ˆqueuedï¼‰ | âœ… æ˜¾ç¤º | åœ¨ reserved_tasks ä¸­ |
| æ‰§è¡Œä¸­ï¼ˆrunningï¼‰ | âœ… æ˜¾ç¤º | åœ¨ active_tasks ä¸­ |
| å·²å®Œæˆï¼ˆsuccess/failedï¼‰ | âŒ **ä¸æ˜¾ç¤º** | ä¸åœ¨ inspect ç»“æœä¸­ |

### æ•°æ®ç•™å­˜æƒ…å†µ

1. **Celeryç»“æœï¼ˆRedisï¼‰**
   ```python
   # config.py ç¬¬51è¡Œ
   'result_expires': 86400 * LOG_RETENTION_DAYS,  # é»˜è®¤7å¤©
   ```
   - âœ… ä»»åŠ¡ç»“æœåœ¨Redisä¸­ä¿ç•™7å¤©
   - âœ… å¯ä»¥é€šè¿‡ `AsyncResult(task_id)` æŸ¥è¯¢
   - âŒ ä½†æ²¡æœ‰åŠæ³•åˆ—å‡ºæ‰€æœ‰å†å²ä»»åŠ¡ID

2. **æ—¥å¿—æ–‡ä»¶**
   ```python
   # tasks.py ç¬¬63è¡Œ
   log_file = f"{DATA_DIR}/logs/{task_id}.log"
   ```
   - âœ… æ¯ä¸ªä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶éƒ½ä¿ç•™
   - âœ… æ–‡ä»¶è·¯å¾„ï¼š`data/logs/{task_id}.log`
   - âŒ ä½†åªæœ‰ä»»åŠ¡IDï¼Œæ— æ³•è·å–å…¶ä»–å…ƒæ•°æ®ï¼ˆæäº¤æ—¶é—´ã€ç”¨æˆ·ã€è„šæœ¬ç­‰ï¼‰

3. **ä¸Šä¼ æ–‡ä»¶**
   ```python
   # app.py ç¬¬143-147è¡Œ
   timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
   saved_filename = f"{timestamp}-{filename}"
   upload_path = f"{DATA_DIR}/uploads/{saved_filename}"
   ```
   - âœ… ä¸Šä¼ çš„ä»£ç åŒ…ä¿å­˜åœ¨ uploads ç›®å½•
   - âŒ ä»»åŠ¡å®Œæˆåä¼šè¢«åˆ é™¤ï¼ˆtasks.py ç¬¬261-266è¡Œï¼‰

## ğŸ¯ ç”¨æˆ·éœ€æ±‚

### åœºæ™¯1ï¼šæŸ¥çœ‹æœ€è¿‘çš„æ„å»ºå†å²

```
ç”¨æˆ·ç™»å½•Webç•Œé¢ï¼Œå¸Œæœ›çœ‹åˆ°ï¼š
- ä»Šå¤©æäº¤çš„æ‰€æœ‰ä»»åŠ¡
- æ¯ä¸ªä»»åŠ¡çš„çŠ¶æ€ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
- ä»»åŠ¡çš„æäº¤æ—¶é—´ã€æ‰§è¡Œæ—¶é•¿
- å¯ä»¥ç‚¹å‡»æŸ¥çœ‹æ—¥å¿—
```

### åœºæ™¯2ï¼šè°ƒè¯•å¤±è´¥çš„æ„å»º

```
å¼€å‘è€…æƒ³çŸ¥é“ï¼š
- ä¸Šæ¬¡å¤±è´¥æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ
- å¤±è´¥çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
- æœ€è¿‘å‡ æ¬¡æ„å»ºçš„è¶‹åŠ¿ï¼ˆæ˜¯å¦é¢‘ç¹å¤±è´¥ï¼‰
```

### åœºæ™¯3ï¼šç»Ÿè®¡åˆ†æ

```
å›¢é˜Ÿleaderæƒ³äº†è§£ï¼š
- æœ¬å‘¨çš„æ„å»ºæ¬¡æ•°
- æˆåŠŸç‡æ˜¯å¤šå°‘
- å¹³å‡æ„å»ºæ—¶é•¿
- å“ªä¸ªåˆ†æ”¯æ„å»ºæœ€é¢‘ç¹
```

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šåŸºäºæ—¥å¿—æ–‡ä»¶çš„ç®€å•å®ç°ï¼ˆæ¨èâ­â­â­ï¼‰

#### ä¼˜ç‚¹
- âœ… æ— éœ€é¢å¤–ä¾èµ–
- âœ… å®ç°ç®€å•
- âœ… åˆ©ç”¨ç°æœ‰æ—¥å¿—æ–‡ä»¶

#### å®ç°æ€è·¯

1. **æ‰«ææ—¥å¿—ç›®å½•è·å–ä»»åŠ¡åˆ—è¡¨**
   ```python
   import os
   import re
   from datetime import datetime

   def list_all_jobs():
       """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡ï¼ˆæ´»è·ƒ + å†å²ï¼‰"""
       jobs = []

       # 1. ä»æ—¥å¿—æ–‡ä»¶è·å–å†å²ä»»åŠ¡
       log_dir = f"{DATA_DIR}/logs"
       for log_file in os.listdir(log_dir):
           if log_file.endswith('.log'):
               job_id = log_file[:-4]  # å»æ‰ .log åç¼€
               job_info = parse_job_from_log(job_id)
               jobs.append(job_info)

       # 2. æ›´æ–°æ´»è·ƒä»»åŠ¡çš„çŠ¶æ€ï¼ˆè¦†ç›–æ—¥å¿—ä¸­çš„çŠ¶æ€ï¼‰
       inspect = celery_app.control.inspect()
       active_ids = set()
       for tasks in [inspect.active(), inspect.scheduled(), inspect.reserved()]:
           if tasks:
               for worker, task_list in tasks.items():
                   for task in task_list:
                       active_ids.add(task['id'])

       # 3. åˆå¹¶ç»“æœ
       for job in jobs:
           if job['job_id'] in active_ids:
               # æ›´æ–°ä¸ºå®æ—¶çŠ¶æ€
               result = AsyncResult(job['job_id'], app=celery_app)
               job['status'] = get_real_status(result)

       # 4. æŒ‰æ—¶é—´æ’åº
       jobs.sort(key=lambda x: x['start_time'], reverse=True)

       return jobs
   ```

2. **ä»æ—¥å¿—æ–‡ä»¶è§£æå…ƒæ•°æ®**
   ```python
   def parse_job_from_log(job_id):
       """ä»æ—¥å¿—æ–‡ä»¶è§£æä»»åŠ¡ä¿¡æ¯"""
       log_file = f"{DATA_DIR}/logs/{job_id}.log"

       if not os.path.exists(log_file):
           return None

       metadata = {
           'job_id': job_id,
           'status': 'unknown',
           'start_time': None,
           'end_time': None,
           'duration': None,
           'mode': 'unknown',
           'user': 'unknown',
           'exit_code': None
       }

       with open(log_file, 'r', encoding='utf-8', errors='replace') as f:
           content = f.read()

           # è§£æå¼€å§‹æ—¶é—´
           match = re.search(r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\].*å¼€å§‹æ—¶é—´', content)
           if match:
               metadata['start_time'] = match.group(1)

           # è§£ææ¨¡å¼
           match = re.search(r'æ¨¡å¼: (\w+)', content)
           if match:
               metadata['mode'] = match.group(1)

           # è§£ææäº¤è€…
           match = re.search(r'æäº¤è€…: (\S+)', content)
           if match:
               metadata['user'] = match.group(1)

           # è§£æç»“æŸæ—¶é—´
           match = re.search(r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\].*ç»“æŸæ—¶é—´', content)
           if match:
               metadata['end_time'] = match.group(1)

           # è§£æè€—æ—¶
           match = re.search(r'æ€»è€—æ—¶: ([\d.]+) ç§’', content)
           if match:
               metadata['duration'] = float(match.group(1))

           # è§£æé€€å‡ºç 
           match = re.search(r'é€€å‡ºç : (-?\d+)', content)
           if match:
               metadata['exit_code'] = int(match.group(1))

           # åˆ¤æ–­çŠ¶æ€
           if 'âœ“ æ„å»ºæˆåŠŸ' in content:
               metadata['status'] = 'success'
           elif 'âœ— æ„å»ºå¤±è´¥' in content:
               metadata['status'] = 'failed'
           elif 'âœ— ä»»åŠ¡è¶…æ—¶' in content:
               metadata['status'] = 'timeout'
           elif 'âœ— ä»»åŠ¡æ‰§è¡Œé”™è¯¯' in content:
               metadata['status'] = 'error'
           elif 'ä»»åŠ¡å¼‚å¸¸ç»ˆæ­¢' in content:
               metadata['status'] = 'error'
           else:
               metadata['status'] = 'running'

       return metadata
   ```

3. **æ·»åŠ åˆ†é¡µå’Œè¿‡æ»¤**
   ```python
   @app.route('/api/jobs/history', methods=['GET'])
   @require_auth
   def get_job_history():
       """è·å–ä»»åŠ¡å†å²ï¼ˆæ”¯æŒåˆ†é¡µå’Œè¿‡æ»¤ï¼‰"""
       # è·å–å‚æ•°
       page = request.args.get('page', 1, type=int)
       per_page = request.args.get('per_page', 20, type=int)
       status = request.args.get('status')  # success, failed, running
       user = request.args.get('user')
       mode = request.args.get('mode')  # rsync, upload, git

       # è·å–æ‰€æœ‰ä»»åŠ¡
       all_jobs = list_all_jobs()

       # è¿‡æ»¤
       if status:
           all_jobs = [j for j in all_jobs if j['status'] == status]
       if user:
           all_jobs = [j for j in all_jobs if j['user'] == user]
       if mode:
           all_jobs = [j for j in all_jobs if j['mode'] == mode]

       # åˆ†é¡µ
       total = len(all_jobs)
       start = (page - 1) * per_page
       end = start + per_page
       jobs = all_jobs[start:end]

       return jsonify({
           'jobs': jobs,
           'total': total,
           'page': page,
           'per_page': per_page,
           'pages': (total + per_page - 1) // per_page
       })
   ```

4. **æ›´æ–°Webç•Œé¢**
   ```javascript
   // æ·»åŠ å†å²è®°å½•æ ‡ç­¾é¡µ
   async function loadAllJobs() {
       const response = await apiCall('/api/jobs/history?per_page=50');
       const data = await response.json();

       // æ˜¾ç¤ºæ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬å†å²ï¼‰
       displayJobs(data.jobs);
   }

   // æ·»åŠ è¿‡æ»¤å™¨
   function filterJobs(status) {
       loadJobsWithFilter({ status: status });
   }
   ```

#### æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨ç¼“å­˜é¿å…é¢‘ç¹æ‰«ææ–‡ä»¶ç³»ç»Ÿ
from functools import lru_cache
from datetime import datetime, timedelta

_job_cache = None
_cache_time = None

def list_all_jobs(use_cache=True):
    global _job_cache, _cache_time

    # ç¼“å­˜30ç§’
    if use_cache and _cache_time and (datetime.now() - _cache_time).seconds < 30:
        return _job_cache

    jobs = scan_log_directory()
    _job_cache = jobs
    _cache_time = datetime.now()

    return jobs
```

---

### æ–¹æ¡ˆ2ï¼šä½¿ç”¨SQLiteæŒä¹…åŒ–ï¼ˆç”Ÿäº§æ¨èâ­â­â­â­â­ï¼‰

#### ä¼˜ç‚¹
- âœ… æŸ¥è¯¢é€Ÿåº¦å¿«
- âœ… æ”¯æŒå¤æ‚è¿‡æ»¤å’Œç»Ÿè®¡
- âœ… æ”¯æŒå¤§é‡å†å²è®°å½•
- âœ… æ— éœ€é¢å¤–æœåŠ¡ï¼ˆSQLiteæ˜¯æ–‡ä»¶æ•°æ®åº“ï¼‰

#### æ•°æ®åº“è®¾è®¡

```sql
CREATE TABLE ci_jobs (
    job_id TEXT PRIMARY KEY,
    mode TEXT NOT NULL,           -- rsync, upload, git
    status TEXT NOT NULL,         -- queued, running, success, failed, timeout, error
    script TEXT NOT NULL,         -- æ„å»ºè„šæœ¬
    user TEXT NOT NULL,           -- æäº¤è€…

    -- æ—¶é—´ä¿¡æ¯
    created_at TIMESTAMP NOT NULL,
    started_at TIMESTAMP,
    finished_at TIMESTAMP,
    duration REAL,                -- ç§’

    -- ç»“æœä¿¡æ¯
    exit_code INTEGER,
    error_message TEXT,

    -- ä»£ç ä¿¡æ¯
    workspace TEXT,               -- rsyncæ¨¡å¼çš„workspaceè·¯å¾„
    repo_url TEXT,                -- gitæ¨¡å¼çš„ä»“åº“URL
    branch TEXT,                  -- gitæ¨¡å¼çš„åˆ†æ”¯
    commit_hash TEXT,             -- gitæ¨¡å¼çš„commit

    -- å…¶ä»–
    log_file TEXT,                -- æ—¥å¿—æ–‡ä»¶è·¯å¾„
    metadata TEXT                 -- JSONæ ¼å¼çš„å…¶ä»–å…ƒæ•°æ®
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_jobs_status ON ci_jobs(status);
CREATE INDEX idx_jobs_user ON ci_jobs(user);
CREATE INDEX idx_jobs_created_at ON ci_jobs(created_at DESC);
CREATE INDEX idx_jobs_mode ON ci_jobs(mode);
```

#### å®ç°ä»£ç 

```python
import sqlite3
import json
from datetime import datetime

class JobDatabase:
    def __init__(self, db_path):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ci_jobs (
                job_id TEXT PRIMARY KEY,
                mode TEXT NOT NULL,
                status TEXT NOT NULL,
                script TEXT NOT NULL,
                user TEXT NOT NULL,
                created_at TIMESTAMP NOT NULL,
                started_at TIMESTAMP,
                finished_at TIMESTAMP,
                duration REAL,
                exit_code INTEGER,
                error_message TEXT,
                workspace TEXT,
                repo_url TEXT,
                branch TEXT,
                commit_hash TEXT,
                log_file TEXT,
                metadata TEXT
            )
        ''')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_jobs_status ON ci_jobs(status)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_jobs_created_at ON ci_jobs(created_at DESC)')
        conn.commit()
        conn.close()

    def create_job(self, job_id, job_data):
        """åˆ›å»ºä»»åŠ¡è®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO ci_jobs (
                job_id, mode, status, script, user,
                created_at, log_file, workspace, repo_url, branch, metadata
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            job_id,
            job_data['mode'],
            'queued',
            job_data['script'],
            job_data.get('user', 'anonymous'),
            datetime.now().isoformat(),
            f"{DATA_DIR}/logs/{job_id}.log",
            job_data.get('workspace'),
            job_data.get('repo'),
            job_data.get('branch'),
            json.dumps(job_data)
        ))
        conn.commit()
        conn.close()

    def update_job_status(self, job_id, status, result=None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        if status == 'running':
            cursor.execute('''
                UPDATE ci_jobs
                SET status = ?, started_at = ?
                WHERE job_id = ?
            ''', (status, datetime.now().isoformat(), job_id))

        elif status in ['success', 'failed', 'timeout', 'error']:
            cursor.execute('''
                UPDATE ci_jobs
                SET status = ?, finished_at = ?, duration = ?, exit_code = ?, error_message = ?
                WHERE job_id = ?
            ''', (
                status,
                datetime.now().isoformat(),
                result.get('duration') if result else None,
                result.get('exit_code') if result else None,
                result.get('error') if result else None,
                job_id
            ))

        conn.commit()
        conn.close()

    def get_jobs(self, limit=50, offset=0, filters=None):
        """æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        query = 'SELECT * FROM ci_jobs'
        params = []

        if filters:
            conditions = []
            if filters.get('status'):
                conditions.append('status = ?')
                params.append(filters['status'])
            if filters.get('user'):
                conditions.append('user = ?')
                params.append(filters['user'])
            if filters.get('mode'):
                conditions.append('mode = ?')
                params.append(filters['mode'])

            if conditions:
                query += ' WHERE ' + ' AND '.join(conditions)

        query += ' ORDER BY created_at DESC LIMIT ? OFFSET ?'
        params.extend([limit, offset])

        cursor.execute(query, params)
        rows = cursor.fetchall()

        jobs = [dict(row) for row in rows]
        conn.close()

        return jobs

    def get_stats(self, days=7):
        """è·å–ç»Ÿè®¡æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cutoff = (datetime.now() - timedelta(days=days)).isoformat()

        cursor.execute('''
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as success_count,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_count,
                AVG(CASE WHEN duration IS NOT NULL THEN duration ELSE NULL END) as avg_duration
            FROM ci_jobs
            WHERE created_at > ?
        ''', (cutoff,))

        row = cursor.fetchone()
        conn.close()

        return {
            'total': row[0],
            'success_count': row[1] or 0,
            'failed_count': row[2] or 0,
            'success_rate': (row[1] or 0) / row[0] if row[0] > 0 else 0,
            'avg_duration': row[3]
        }
```

#### é›†æˆåˆ°API

```python
# åˆå§‹åŒ–æ•°æ®åº“
job_db = JobDatabase(f"{DATA_DIR}/jobs.db")

@app.route('/api/jobs/rsync', methods=['POST'])
@require_auth
def create_rsync_job():
    # ... éªŒè¯ä»£ç  ...

    # æäº¤ä»»åŠ¡
    task = execute_build.delay(job_data)

    # è®°å½•åˆ°æ•°æ®åº“
    job_db.create_job(task.id, job_data)

    return jsonify({
        'job_id': task.id,
        'status': 'queued',
        'mode': 'rsync'
    }), 201

@app.route('/api/jobs/history', methods=['GET'])
@require_auth
def get_job_history():
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 20, type=int)

    filters = {
        'status': request.args.get('status'),
        'user': request.args.get('user'),
        'mode': request.args.get('mode')
    }

    jobs = job_db.get_jobs(
        limit=per_page,
        offset=(page - 1) * per_page,
        filters={k: v for k, v in filters.items() if v}
    )

    return jsonify({
        'jobs': jobs,
        'page': page,
        'per_page': per_page
    })

@app.route('/api/stats', methods=['GET'])
@require_auth
def get_stats():
    days = request.args.get('days', 7, type=int)
    stats = job_db.get_stats(days=days)
    return jsonify(stats)
```

#### åœ¨tasks.pyä¸­æ›´æ–°çŠ¶æ€

```python
@celery_app.task(base=BuildTask, bind=True, name='remote_ci.build')
def execute_build(self, job_data):
    task_id = self.request.id

    # æ›´æ–°ä¸ºè¿è¡Œä¸­
    job_db.update_job_status(task_id, 'running')

    try:
        # ... æ‰§è¡Œæ„å»º ...

        # æ›´æ–°ä¸ºæˆåŠŸ/å¤±è´¥
        job_db.update_job_status(task_id, status, result)

        return result
    except Exception as e:
        job_db.update_job_status(task_id, 'error', {'error': str(e)})
        raise
```

---

### æ–¹æ¡ˆ3ï¼šä½¿ç”¨PostgreSQL/MySQLï¼ˆä¼ä¸šçº§â­â­â­â­ï¼‰

#### ä¼˜ç‚¹
- âœ… æ”¯æŒé«˜å¹¶å‘
- âœ… æ›´å¼ºå¤§çš„æŸ¥è¯¢èƒ½åŠ›
- âœ… æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²

#### ç¼ºç‚¹
- âŒ éœ€è¦é¢å¤–çš„æ•°æ®åº“æœåŠ¡
- âŒ å¢åŠ éƒ¨ç½²å¤æ‚åº¦
- âŒ å¯¹äºè½»é‡çº§CIç³»ç»Ÿè¿‡äºå¤æ‚

---

## ğŸ“‹ å®ç°å¯¹æ¯”

| æ–¹æ¡ˆ | å¤æ‚åº¦ | æ€§èƒ½ | æ‰©å±•æ€§ | æ¨èåœºæ™¯ |
|-----|-------|-----|-------|---------|
| æ–¹æ¡ˆ1ï¼ˆæ—¥å¿—æ‰«æï¼‰ | â­ ç®€å• | â­â­ ä¸­ç­‰ | â­â­ ä¸€èˆ¬ | å°å›¢é˜Ÿã€ä»»åŠ¡é‡<1000/å¤© |
| æ–¹æ¡ˆ2ï¼ˆSQLiteï¼‰ | â­â­ ä¸­ç­‰ | â­â­â­â­ å¿« | â­â­â­â­ å¥½ | **æ¨è**ï¼Œé€‚åˆå¤§éƒ¨åˆ†åœºæ™¯ |
| æ–¹æ¡ˆ3ï¼ˆPostgreSQLï¼‰ | â­â­â­ å¤æ‚ | â­â­â­â­â­ å¾ˆå¿« | â­â­â­â­â­ ä¼˜ç§€ | ä¼ä¸šçº§ã€é«˜å¹¶å‘ |

## ğŸ¯ æ¨èå®æ–½æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿä¿®å¤ï¼ˆæ–¹æ¡ˆ1ï¼‰

1. å®ç° `parse_job_from_log()` å‡½æ•°
2. æ·»åŠ  `/api/jobs/history` æ¥å£
3. æ›´æ–°Webç•Œé¢æ˜¾ç¤ºå†å²ä»»åŠ¡
4. **é¢„è®¡å·¥ä½œé‡ï¼š2-3å°æ—¶**

### ç¬¬äºŒé˜¶æ®µï¼šç”Ÿäº§åŒ–ï¼ˆæ–¹æ¡ˆ2ï¼‰

1. è®¾è®¡å¹¶åˆ›å»ºSQLiteæ•°æ®åº“
2. å®ç° `JobDatabase` ç±»
3. åœ¨ä»»åŠ¡åˆ›å»ºå’Œæ›´æ–°æ—¶å†™å…¥æ•°æ®åº“
4. æ·»åŠ ç»Ÿè®¡åˆ†ææ¥å£
5. æ›´æ–°Webç•Œé¢æ”¯æŒè¿‡æ»¤å’Œåˆ†é¡µ
6. **é¢„è®¡å·¥ä½œé‡ï¼š1-2å¤©**

### ç¬¬ä¸‰é˜¶æ®µï¼šä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

1. æ·»åŠ æ•°æ®å¯¼å‡ºåŠŸèƒ½ï¼ˆCSV/Excelï¼‰
2. æ·»åŠ è¶‹åŠ¿å›¾è¡¨
3. æ·»åŠ é‚®ä»¶/Webhooké€šçŸ¥
4. **é¢„è®¡å·¥ä½œé‡ï¼š1-2å¤©**

## ğŸš€ å¿«é€Ÿå¼€å§‹

æƒ³è¦ç«‹å³ä¿®å¤è¿™ä¸ªé—®é¢˜ï¼Ÿæˆ‘å¯ä»¥å¸®æ‚¨å®ç°ï¼š

1. **æ–¹æ¡ˆ1ï¼ˆå¿«é€Ÿï¼‰**ï¼šåŸºäºæ—¥å¿—æ–‡ä»¶çš„å†å²è®°å½•æŸ¥è¯¢
2. **æ–¹æ¡ˆ2ï¼ˆæ¨èï¼‰**ï¼šåŸºäºSQLiteçš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

è¯·å‘Šè¯‰æˆ‘æ‚¨å¸Œæœ›ä½¿ç”¨å“ªä¸ªæ–¹æ¡ˆï¼Œæˆ‘å°†ç«‹å³å¼€å§‹å®ç°ï¼
